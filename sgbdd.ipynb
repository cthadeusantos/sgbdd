{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Rzl0zUaVFupH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721033069,"user_tz":180,"elapsed":1209,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"2176fc32-0a02-48ac-d3d1-38276ed85a21"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'derby.log': No such file or directory\n","rm: cannot remove 'create_tables.hql': No such file or directory\n","rm: cannot remove 'load_data.hql': No such file or directory\n"]}],"source":["# APAGA OS ARQUIVOS PARA REINICIAR o projeto\n","!rm -rf metastore_db/\n","!rm -rf hadoop-3.3.6/\n","!rm -rf /usr/local/hive/apache-hive-3.1.3/\n","!rm -rf /usr/local/spark-3.5.6-bin-hadoop3/\n","#!rm apache-hive-3.1.3-bin.tar.*\n","#!rm hadoop-3.3.6.tar.*\n","#!rm spark-3.5.6-bin-hadoop3.*\n","!rm -rf apache-hive-3.1.3-bin/\n","!rm derby.log\n","!rm -rf /usr/local/hadoop/hadoop-3.3.6\n","!rm -rf spark-3.5.6-bin-hadoop3/\n","!rm -rf spark-warehouse/\n","!rm create_tables.hql\n","!rm load_data.hql"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKue0FLEFvR-","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721060283,"user_tz":180,"elapsed":20504,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"bff2045d-dd14-4907-a5f7-5b9e73ceb944"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Como deixar meus arquivos salvo no Colab?\n","# Olá, Joyce, tudo bem?\n","# Infelizmente fazer um upload e deixar salvo direto não é possível,\n","# pois o Colab disponibiliza uma Vm temporária para ser usada.\n","# Mas existe a opção de deixar os arquivos em uma pasta do drive e acessar essa\n","# pasta diretamente do Google Colab.\n","# Assim não seria necessário ter que fazer o upload a cada vez que for usar.\n","# Para isso precisamos abrir o drive onde vamos acessar esses dados com o comando abaixo:\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3_TKJ5OtaLW"},"outputs":[],"source":["# Instala e configura as variaveis de ambiente do Java\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] += \":/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0yP0L2yUtoJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721127700,"user_tz":180,"elapsed":39531,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"536a2f7d-91a8-44b2-bb9c-08561e3fefa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-23 23:24:48--  https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n","Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n","Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 730107476 (696M) [application/x-gzip]\n","Saving to: ‘hadoop-3.3.6.tar.gz’\n","\n","hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  30.1MB/s    in 24s     \n","\n","2025-06-23 23:25:12 (29.0 MB/s) - ‘hadoop-3.3.6.tar.gz’ saved [730107476/730107476]\n","\n"]}],"source":["# Instala e configura as variaveis de ambiente do Hadoop\n","!wget -nc https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n","!tar -xzf hadoop-3.3.6.tar.gz\n","!mv hadoop-3.3.6 /usr/local/hadoop\n","\n","os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n","os.environ[\"PATH\"] += \":/usr/local/hadoop/bin\"\n","os.environ[\"PATH\"] += \":/usr/local/hadoop/sbin\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1750721340170,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"},"user_tz":180},"id":"BZqT2JuCuMZe","outputId":"c97fefac-cdf6-405e-cbe5-1163117f356c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /usr/local/hadoop/etc/hadoop/core-site.xml\n"]}],"source":["# Configurar Hadoop (modo pseudo-distribuído)\n","# Editar o arquivo core-site.xml\n","# Desativar o HDFS para simplificar no Colab (sem daemon rodando).\n","# Rodamos sobre o sistema de arquivos local (file:///).\n","\n","%%writefile /usr/local/hadoop/etc/hadoop/core-site.xml\n","<?xml version=\"1.0\"?>\n","<configuration>\n","  <property>\n","    <name>fs.defaultFS</name>\n","    <value>file:///</value>\n","  </property>\n","</configuration>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"FPbl79I8uQgF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721362212,"user_tz":180,"elapsed":19119,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"459930e2-cc48-458d-e38f-3c7006e0a24d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-23 23:29:03--  https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n","Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n","Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 326940667 (312M) [application/x-gzip]\n","Saving to: ‘apache-hive-3.1.3-bin.tar.gz’\n","\n","apache-hive-3.1.3-b 100%[===================>] 311.79M  19.9MB/s    in 15s     \n","\n","2025-06-23 23:29:18 (21.4 MB/s) - ‘apache-hive-3.1.3-bin.tar.gz’ saved [326940667/326940667]\n","\n"]}],"source":["# Instala HIVE e configura as variaveis de ambiente\n","!wget -nc https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n","!tar -xzf apache-hive-3.1.3-bin.tar.gz\n","!mv apache-hive-3.1.3-bin /usr/local/hive\n","\n","os.environ[\"HIVE_HOME\"] = \"/usr/local/hive\"\n","os.environ[\"PATH\"] += \":/usr/local/hive/bin\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1ptV6pTwu7Az","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721412763,"user_tz":180,"elapsed":11482,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"97f73781-eb74-49ac-b311-e613d8b22e30"},"outputs":[{"output_type":"stream","name":"stdout","text":["SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Metastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\n","Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n","Metastore connection User:\t APP\n","Starting metastore schema initialization to 3.1.0\n","Initialization script hive-schema-3.1.0.derby.sql\n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n","\n"," \n"," \n"," \n","\n"," \n"," \n"," \n","\n","\n"," \n"," \n"," \n","\n"," \n"," \n"," \n","\n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n","\n"," \n"," \n"," \n","\n"," \n"," \n"," \n","\n"," \n"," \n","\n"," \n"," \n"," \n"," \n","\n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n","\n","\n"," \n","\n","\n","\n"," \n","\n","\n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n"," \n"," \n"," \n"," \n"," \n","\n"," \n","\n","\n","Initialization script completed\n","schemaTool completed\n"]}],"source":["!schematool -dbType derby -initSchema"]},{"cell_type":"code","source":["%%bash\n","#from pickle import STRING\n","# Cria o banco de dados e as tabelas\n","hive -e \"CREATE DATABASE IF NOT EXISTS polroute;\"\n","\n","hive -e \"USE polroute;\"\n","\n","hive -e \"\n","CREATE TABLE IF NOT EXISTS crime (\n","    id INT,\n","    total_feminicide INT,\n","    total_homicide INT,\n","    total_felony_murder INT,\n","    total_bodily_harm INT,\n","    total_theft_cellphone INT,\n","    total_armed_robbery_cellphone INT,\n","    total_theft_auto INT,\n","    total_armed_robbery_auto INT,\n","    segment_id INT,\n","    time_id INT\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1');\n","\n","CREATE TABLE IF NOT EXISTS district (\n","    id INT,\n","    name STRING,\n","    geometry STRING\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1');\n","\n","CREATE TABLE IF NOT EXISTS neighborhood (\n","    id INT,\n","    name STRING,\n","    geometry STRING\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1',\n","              'serialization.encoding'='UTF-8');\n","\n","CREATE TABLE IF NOT EXISTS segment (\n","    id INT,\n","    geometry STRING,\n","    oneway BOOLEAN,\n","    length DOUBLE,\n","    final_vertice_id INT,\n","    start_vertice_id INT\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1');\n","\n","CREATE TABLE IF NOT EXISTS timex (\n","    id INT,\n","    period STRING,\n","    day INT,\n","    month INT,\n","    year INT,\n","    weekday STRING\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1');\n","\n","CREATE TABLE IF NOT EXISTS vertice (\n","    id INT,\n","    label INT,\n","    district_id INT,\n","    neighborhood_id INT,\n","    zone_id INT\n","    )\n","ROW FORMAT DELIMITED\n","FIELDS TERMINATED BY ';'\n","STORED AS TEXTFILE\n","TBLPROPERTIES('skip.header.line.count'='1');\n","\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4izMkclI7cZM","executionInfo":{"status":"ok","timestamp":1750721481313,"user_tz":180,"elapsed":52167,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"203971e5-308f-45f7-c72a-740543a18c79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = fd14a313-6c9b-4f5f-a55e-5aa717f689ef\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = 6436cbb9-2cca-4320-8d00-08aa5b2d9cc9\n","OK\n","Time taken: 1.41 seconds\n","SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = 7f1c4355-123b-4e06-bb9d-c65869fad154\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = fe0e7aaf-ed89-45b0-9b2b-5311b0b122f1\n","OK\n","Time taken: 1.326 seconds\n","SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = fe8591a7-7b34-4410-ae5a-6663312e38f6\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = 7c0be8d9-00a9-42f2-b2bf-83c232e7c6f9\n","OK\n","Time taken: 2.292 seconds\n","OK\n","Time taken: 0.118 seconds\n","OK\n","Time taken: 0.077 seconds\n","OK\n","Time taken: 0.101 seconds\n","OK\n","Time taken: 0.093 seconds\n","OK\n","Time taken: 0.092 seconds\n"]}]},{"cell_type":"code","source":["%%bash\n","#hive -e \"USE polroute;\"\n","hive -e \"SHOW TABLES;\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PeTlSDm757de","executionInfo":{"status":"ok","timestamp":1749518084771,"user_tz":180,"elapsed":12848,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"65f2f51a-274c-44ae-82ce-9d20dff4514d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["crime\n","district\n","neighborhood\n","segment\n","timex\n","vertice\n"]},{"output_type":"stream","name":"stderr","text":["SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = 317793a3-0402-48b5-800b-dff4663c2d4b\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = 03f78add-7fd0-4e63-9aaf-ecde663278ea\n","OK\n","Time taken: 1.379 seconds, Fetched: 6 row(s)\n"]}]},{"cell_type":"code","source":["###################################################\n","# WARNING WARNING WARNING WARNING WARNING WARNING #\n","# WARNING - Só usar se quiser recriar a tabela.   #\n","# WARNING WARNING WARNING WARNING WARNING WARNING #\n","###################################################\n","!hive -e \"\"\"DROP DATABASE polroute CASCADE;\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dx8Kiv92_4NT","executionInfo":{"status":"ok","timestamp":1749518255560,"user_tz":180,"elapsed":12769,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"00f2e521-21e6-4a7d-c1dd-e4abb315e6e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = d5e62a75-0bea-493b-845b-7232e14a7e58\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = cc4137e0-cadb-4c39-ac8e-e584b20afc20\n","OK\n","Time taken: 1.857 seconds\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6qj_CF-Gz01","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750721523801,"user_tz":180,"elapsed":29206,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"68f65bd5-bec4-40ec-bc03-3d3b0aa1c6b2"},"outputs":[{"output_type":"stream","name":"stderr","text":["SLF4J: Class path contains multiple SLF4J bindings.\n","SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n","SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n","SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n","Hive Session ID = 8d6c5988-b6b0-4e46-b040-5e7d27f31902\n","\n","Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n","Hive Session ID = 3f78ede1-f54f-4faf-8d81-777b6aa48ada\n","Loading data to table default.timex\n","OK\n","Time taken: 4.293 seconds\n","Loading data to table default.crime\n","OK\n","Time taken: 4.092 seconds\n","Loading data to table default.district\n","OK\n","Time taken: 1.074 seconds\n","Loading data to table default.neighborhood\n","OK\n","Time taken: 1.172 seconds\n","Loading data to table default.vertice\n","OK\n","Time taken: 0.943 seconds\n","Loading data to table default.segment\n","OK\n","Time taken: 2.175 seconds\n"]}],"source":["%%bash\n","hive -e \"\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/time.csv' INTO TABLE timex;\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/crime.csv' INTO TABLE crime;\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/district.csv' INTO TABLE district;\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/neighborhood.csv' INTO TABLE neighborhood;\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/vertice.csv' INTO TABLE vertice;\n","LOAD DATA LOCAL INPATH '/content/drive/MyDrive/data/segment.csv' INTO TABLE segment;\n","\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWZsNHaFJW1U"},"outputs":[],"source":["import multiprocessing\n","import os\n","print(multiprocessing.cpu_count())\n","print(os.cpu_count())\n","print(len(os.sched_getaffinity(0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdIuJOGe5tjS"},"outputs":[],"source":["!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -nc -q https://downloads.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz\n","!tar xf spark-3.5.6-bin-hadoop3.tgz\n","!pip install -q findspark pyspark\n"]},{"cell_type":"code","source":["import os\n","import findspark\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.6-bin-hadoop3\"\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n","    .getOrCreate()\n","\n","spark\n"],"metadata":{"id":"9I4e3VQa5_-1","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"ok","timestamp":1750721577894,"user_tz":180,"elapsed":12440,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"b9544309-c16b-4b7f-cc7c-ca1057dcce17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x79052eb58850>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4ce07fc12892:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.6</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PolRoute-DS</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Subindo os arquivos CSV manualmente para o ambiente do Colab\n","\n","from pyspark.sql import functions as F\n","\n","# Exemplo de carregamento\n","# Add the delimiter option to correctly parse CSV files\n","crime = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/crime.csv\")\n","segment = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/segment.csv\")\n","vertice = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/vertice.csv\")\n","district = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/district.csv\")\n","neighborhood = spark.read.option(\"encoding\", \"UTF-8\").option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/neighborhood.csv\")\n","timex = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/content/drive/MyDrive/data/time.csv\")\n","\n","# Crie o schema no Spark SQL (equivalente ao Hive)\n","\n","crime.createOrReplaceTempView(\"crime\")\n","segment.createOrReplaceTempView(\"segment\")\n","vertice.createOrReplaceTempView(\"vertice\")\n","district.createOrReplaceTempView(\"district\")\n","neighborhood.createOrReplaceTempView(\"neighborhood\")\n","timex.createOrReplaceTempView(\"timex\")"],"metadata":{"id":"_cFH1b_r6DK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 1         #\n","#                          #\n","############################\n","\n","query1 = spark.sql(\"\"\"\n","WITH filtered_district AS (\n","    SELECT id FROM district WHERE name = 'IGUATEMI'\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year = 2016\n","),\n","start_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.start_vertice_id = v.id\n","),\n","end_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.final_vertice_id = v.id\n","),\n","segment_with_district AS (\n","    SELECT DISTINCT segment_id\n","    FROM (\n","        SELECT * FROM start_segments\n","        UNION\n","        SELECT * FROM end_segments\n","    ) sd\n","    JOIN filtered_district d ON sd.district_id = d.id\n",")\n","SELECT\n","    c.segment_id,\n","    SUM(c.total_feminicide) AS total_feminicide,\n","    SUM(c.total_homicide) AS total_homicide,\n","    SUM(c.total_felony_murder) AS total_felony_murder,\n","    SUM(c.total_bodily_harm) AS total_bodily_harm,\n","    SUM(c.total_theft_cellphone) AS total_theft_cellphone,\n","    SUM(c.total_armed_robbery_cellphone) AS total_armed_robbery_cellphone,\n","    SUM(c.total_theft_auto) AS total_theft_auto,\n","    SUM(c.total_armed_robbery_auto) AS total_armed_robbery_auto\n","FROM\n","    crime c\n","JOIN\n","    segment_with_district sd ON c.segment_id = sd.segment_id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    c.segment_id\n","\"\"\")\n","query1.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RexjNn0c74Zy","executionInfo":{"status":"ok","timestamp":1749424086742,"user_tz":180,"elapsed":35276,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"3f75a729-1839-4b5a-91fb-e9c0dffd1177","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----------------+--------------+-------------------+-----------------+---------------------+-----------------------------+----------------+------------------------+\n","|segment_id|total_feminicide|total_homicide|total_felony_murder|total_bodily_harm|total_theft_cellphone|total_armed_robbery_cellphone|total_theft_auto|total_armed_robbery_auto|\n","+----------+----------------+--------------+-------------------+-----------------+---------------------+-----------------------------+----------------+------------------------+\n","|    100289|             0.0|           0.0|                0.0|              0.0|                  5.0|                          0.0|             5.0|                     0.0|\n","|    100291|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     0.0|\n","|    100294|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     0.0|\n","|    100296|             0.0|           0.0|                0.0|              0.0|                  5.0|                          0.0|             0.0|                     0.0|\n","|    100297|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             5.0|                     5.0|\n","|    100298|             0.0|           0.0|                0.0|              0.0|                  5.0|                          0.0|             0.0|                     0.0|\n","|    100300|             0.0|           0.0|                0.0|              0.0|                  0.0|                         10.0|             0.0|                     0.0|\n","|    100301|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                    10.0|\n","|    100303|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             5.0|                     0.0|\n","|    100307|             0.0|           0.0|                0.0|              0.0|                  0.0|                          0.0|             0.0|                     5.0|\n","|    100311|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     0.0|\n","|    100316|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     0.0|\n","|    100317|             0.0|           0.0|                0.0|              0.0|                  5.0|                          0.0|             0.0|                     0.0|\n","|    100318|             0.0|           0.0|                0.0|              0.0|                  0.0|                          0.0|             0.0|                     5.0|\n","|    100327|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     0.0|\n","|    100328|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     5.0|\n","|    100329|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     5.0|\n","|    100332|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     5.0|\n","|    100333|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             0.0|                     5.0|\n","|    113117|             0.0|           0.0|                0.0|              0.0|                  0.0|                          5.0|             5.0|                     5.0|\n","+----------+----------------+--------------+-------------------+-----------------+---------------------+-----------------------------+----------------+------------------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 1         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lit, count, sum, col\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query1 = spark.sql(\"\"\"\n","WITH filtered_district AS (\n","    SELECT id FROM district WHERE name = 'IGUATEMI'\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year = 2016\n","),\n","start_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.start_vertice_id = v.id\n","),\n","end_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.final_vertice_id = v.id\n","),\n","segment_with_district AS (\n","    SELECT DISTINCT segment_id\n","    FROM (\n","        SELECT * FROM start_segments\n","        UNION\n","        SELECT * FROM end_segments\n","    ) sd\n","    JOIN filtered_district d ON sd.district_id = d.id\n",")\n","SELECT\n","    c.segment_id,\n","    SUM(c.total_feminicide) AS total_feminicide,\n","    SUM(c.total_homicide) AS total_homicide,\n","    SUM(c.total_felony_murder) AS total_felony_murder,\n","    SUM(c.total_bodily_harm) AS total_bodily_harm,\n","    SUM(c.total_theft_cellphone) AS total_theft_cellphone,\n","    SUM(c.total_armed_robbery_cellphone) AS total_armed_robbery_cellphone,\n","    SUM(c.total_theft_auto) AS total_theft_auto,\n","    SUM(c.total_armed_robbery_auto) AS total_armed_robbery_auto\n","FROM\n","    crime c\n","JOIN\n","    segment_with_district sd ON c.segment_id = sd.segment_id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    c.segment_id;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query1-1cpu-new.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query1.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query1-2cpu-new.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query1.collect()\n","        end = time.time()\n","        duration = round(end - start, 3)\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query1\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query1-new\"))\n","\n","# Cria DataFrame com totais\n","totais_df = query1.agg(\n","    count(\"*\").alias(\"total_segments\"),\n","    sum(\"total_feminicide\").alias(\"total_feminicide\"),\n","    sum(\"total_homicide\").alias(\"total_homicide\"),\n","    sum(\"total_felony_murder\").alias(\"total_felony_murder\"),\n","    sum(\"total_bodily_harm\").alias(\"total_bodily_harm\"),\n","    sum(\"total_theft_cellphone\").alias(\"total_theft_cellphone\"),\n","    sum(\"total_armed_robbery_cellphone\").alias(\"total_armed_robbery_cellphone\"),\n","    sum(\"total_theft_auto\").alias(\"total_theft_auto\"),\n","    sum(\"total_armed_robbery_auto\").alias(\"total_armed_robbery_auto\")\n",").withColumn(\"segment_id\", lit(\"TOTAL\"))\n","\n","# Adiciona linha de totais ao resultado original\n","#resultado_final = query1.withColumn(\"total_segments\", lit(None).cast(\"int\")) \\\n","#    .select(\"segment_id\", \"total_segments\", *[c for c in query1.columns if c != \"segment_id\"]) \\\n","#    .unionByName(totais_df) \\\n","#    .orderBy(\"segment_id\")\n","\n","totais_df.collect()\n","# Para CSV\n","(totais_df\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/soma_final_query1\"))\n","\n"],"metadata":{"id":"a8RzNqoz-ar2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 2         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, count, lit\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query2 = spark.sql(\"\"\"\n","WITH filtered_district AS (\n","    SELECT id FROM district WHERE name = 'IGUATEMI'\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year BETWEEN 2006 AND 2016\n","),\n","start_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.start_vertice_id = v.id\n","),\n","end_segments AS (\n","    SELECT s.id AS segment_id, v.district_id\n","    FROM segment s\n","    JOIN vertice v ON s.final_vertice_id = v.id\n","),\n","segment_with_district AS (\n","    SELECT DISTINCT segment_id\n","    FROM (\n","        SELECT * FROM start_segments\n","        UNION ALL\n","        SELECT * FROM end_segments\n","    ) seg\n","    JOIN filtered_district d ON seg.district_id = d.id\n",")\n","SELECT\n","    c.segment_id,\n","    SUM(c.total_feminicide) AS total_feminicide,\n","    SUM(c.total_homicide) AS total_homicide,\n","    SUM(c.total_felony_murder) AS total_felony_murder,\n","    SUM(c.total_bodily_harm) AS total_bodily_harm,\n","    SUM(c.total_theft_cellphone) AS total_theft_cellphone,\n","    SUM(c.total_armed_robbery_cellphone) AS total_armed_robbery_cellphone,\n","    SUM(c.total_theft_auto) AS total_theft_auto,\n","    SUM(c.total_armed_robbery_auto) AS total_armed_robbery_auto\n","FROM\n","    crime c\n","JOIN\n","    segment_with_district sd ON c.segment_id = sd.segment_id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    c.segment_id;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query2-1cpu-new.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query2.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query2-2cpu-new.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query2.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query2\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query2-new\"))\n","\n","# Cria a agregação total a partir dos resultados de query2\n","crime_totals = query2.agg(\n","    count(\"segment_id\").alias(\"total_segments\"),\n","    sum(\"total_feminicide\").alias(\"total_feminicide\"),\n","    sum(\"total_homicide\").alias(\"total_homicide\"),\n","    sum(\"total_felony_murder\").alias(\"total_felony_murder\"),\n","    sum(\"total_bodily_harm\").alias(\"total_bodily_harm\"),\n","    sum(\"total_theft_cellphone\").alias(\"total_theft_cellphone\"),\n","    sum(\"total_armed_robbery_cellphone\").alias(\"total_armed_robbery_cellphone\"),\n","    sum(\"total_theft_auto\").alias(\"total_theft_auto\"),\n","    sum(\"total_armed_robbery_auto\").alias(\"total_armed_robbery_auto\")\n",")\n","# Adiciona uma coluna de identificação\n","crime_totals = crime_totals.withColumn(\"aggregation_type\", lit(\"TOTAL_PERIOD_2006_2016\"))\n","crime_totals.collect()\n","# Para CSV\n","(crime_totals\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/soma_final_query2\"))\n","\n","#spark.stop()"],"metadata":{"id":"6iJsq200rd8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 3         #\n","#                          #\n","############################\n","\n","#query3 = spark.sql(\"\"\"\n","#SELECT\n","#    c.segment_id,\n","#    SUM(c.total_theft_cellphone + c.total_armed_robbery_cellphone) AS total_cellphone_crimes,\n","#    SUM(c.total_theft_auto + c.total_armed_robbery_auto) AS total_auto_crimes\n","#FROM\n","#    crime c\n","#JOIN\n","#    segment s ON c.segment_id = s.id\n","#JOIN\n","#    vertice v ON s.start_vertice_id = v.id OR s.final_vertice_id = v.id\n","#JOIN\n","#    neighborhood n ON v.neighborhood_id = n.id\n","#JOIN\n","#    timex t ON c.time_id = t.id\n","#WHERE\n","#    n.name = 'Santa Efig�nia'\n","#    AND t.year = 2015\n","#GROUP BY\n","#    c.segment_id\n","#ORDER BY\n","#    c.segment_id;\n","#\"\"\")\n","\n","query3 = spark.sql(\"\"\"\n","WITH filtered_neighborhood AS (\n","    SELECT id FROM neighborhood WHERE name = 'Santa Efig�nia'\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year = 2015\n","),\n","start_segments AS (\n","    SELECT s.id AS segment_id, v.neighborhood_id\n","    FROM segment s\n","    JOIN vertice v ON s.start_vertice_id = v.id\n","),\n","end_segments AS (\n","    SELECT s.id AS segment_id, v.neighborhood_id\n","    FROM segment s\n","    JOIN vertice v ON s.final_vertice_id = v.id\n","),\n","segment_with_neighborhood AS (\n","    SELECT DISTINCT segment_id\n","    FROM (\n","        SELECT * FROM start_segments\n","        UNION ALL\n","        SELECT * FROM end_segments\n","    ) sv\n","    JOIN filtered_neighborhood fn ON sv.neighborhood_id = fn.id\n",")\n","SELECT\n","    c.segment_id,\n","    SUM(c.total_theft_cellphone + c.total_armed_robbery_cellphone) AS total_cellphone_crimes,\n","    SUM(c.total_theft_auto + c.total_armed_robbery_auto) AS total_auto_crimes\n","FROM\n","    crime c\n","JOIN\n","    segment_with_neighborhood swn ON c.segment_id = swn.segment_id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    c.segment_id;\n","\"\"\")\n","query3.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6A_bcB451XG3","executionInfo":{"status":"ok","timestamp":1749518434138,"user_tz":180,"elapsed":32473,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"5bdf9830-ee91-4180-ccaa-9619ad26282e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+----------------------+-----------------+\n","|segment_id|total_cellphone_crimes|total_auto_crimes|\n","+----------+----------------------+-----------------+\n","|    106224|                   5.0|              0.0|\n","|    106227|                  15.0|              0.0|\n","|    106233|                  35.0|              0.0|\n","|    106240|                  15.0|              0.0|\n","|    106241|                  30.0|              0.0|\n","|    106242|                   5.0|              0.0|\n","|    106281|                  70.0|              5.0|\n","|    109024|                  80.0|              5.0|\n","|    109032|                 520.0|              5.0|\n","|    109033|                  60.0|              0.0|\n","|    109035|                  50.0|              0.0|\n","|    127529|                  60.0|              0.0|\n","|    127530|                   5.0|              0.0|\n","|    127537|                   5.0|              0.0|\n","|    127538|                  10.0|              0.0|\n","|    127545|                  20.0|              0.0|\n","|    127718|                  40.0|              5.0|\n","|    127719|                  55.0|              0.0|\n","|    127720|                  80.0|              0.0|\n","|    127722|                   5.0|              5.0|\n","+----------+----------------------+-----------------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 3         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import sum, count, lit\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query3 = spark.sql(\"\"\"\n","WITH filtered_neighborhood AS (\n","    SELECT id FROM neighborhood WHERE name = 'Santa Efig�nia'\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year = 2015\n","),\n","start_segments AS (\n","    SELECT s.id AS segment_id, v.neighborhood_id\n","    FROM segment s\n","    JOIN vertice v ON s.start_vertice_id = v.id\n","),\n","end_segments AS (\n","    SELECT s.id AS segment_id, v.neighborhood_id\n","    FROM segment s\n","    JOIN vertice v ON s.final_vertice_id = v.id\n","),\n","segment_with_neighborhood AS (\n","    SELECT DISTINCT segment_id\n","    FROM (\n","        SELECT * FROM start_segments\n","        UNION ALL\n","        SELECT * FROM end_segments\n","    ) sv\n","    JOIN filtered_neighborhood fn ON sv.neighborhood_id = fn.id\n",")\n","SELECT\n","    c.segment_id,\n","    SUM(c.total_theft_cellphone + c.total_armed_robbery_cellphone) AS total_cellphone_crimes,\n","    SUM(c.total_theft_auto + c.total_armed_robbery_auto) AS total_auto_crimes\n","FROM\n","    crime c\n","JOIN\n","    segment_with_neighborhood swn ON c.segment_id = swn.segment_id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    c.segment_id;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query3-1cpu-new.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query3.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query3-2cpu-new.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query3.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query3\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query3-new\"))\n","\n","# Cria a agregação total a partir dos resultados de query3\n","crime_totals = query3.agg(\n","    count(\"segment_id\").alias(\"total_segments\"),\n","    sum(\"total_cellphone_crimes\").alias(\"total_cellphone_crimes\"),\n","    sum(\"total_auto_crimes\").alias(\"total_auto_crimes\")\n",")\n","crime_totals.collect()\n","# Para CSV\n","(crime_totals\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/soma_final_query3\"))\n","#spark.stop()"],"metadata":{"id":"By1-gb2v6fQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M4FqqkA4LSWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 4         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query4 = spark.sql(\"\"\"\n","WITH filtered_segment AS (\n","    SELECT id FROM segment WHERE oneway = TRUE\n","),\n","filtered_timex AS (\n","    SELECT id FROM timex WHERE year = 2012\n",")\n","SELECT\n","    SUM(c.total_feminicide) AS total_feminicide,\n","    SUM(c.total_homicide) AS total_homicide,\n","    SUM(c.total_felony_murder) AS total_felony_murder,\n","    SUM(c.total_bodily_harm) AS total_bodily_harm,\n","    SUM(c.total_theft_cellphone) AS total_theft_cellphone,\n","    SUM(c.total_armed_robbery_cellphone) AS total_armed_robbery_cellphone,\n","    SUM(c.total_theft_auto) AS total_theft_auto,\n","    SUM(c.total_armed_robbery_auto) AS total_armed_robbery_auto\n","FROM\n","    crime c\n","JOIN\n","    filtered_segment s ON c.segment_id = s.id\n","JOIN\n","    filtered_timex t ON c.time_id = t.id;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query4-1cpu.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query4.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query4-2cpu.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query4.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query4\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query4\"))\n","\n","#spark.stop()"],"metadata":{"id":"geOcPsgmLTJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DK3Hq6btLf83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 5         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","# query5 = spark.sql(\"\"\"\n","#WITH filtered_segment AS (\n","#    SELECT id FROM segment WHERE oneway = TRUE\n","#),\n","#filtered_timex AS (\n","#    SELECT id FROM timex WHERE year = 2012\n","#)\n","#SELECT\n","#    SUM(c.total_feminicide) AS total_feminicide,\n","#    SUM(c.total_homicide) AS total_homicide,\n","#    SUM(c.total_felony_murder) AS total_felony_murder,\n","#    SUM(c.total_bodily_harm) AS total_bodily_harm,\n","#    SUM(c.total_theft_cellphone) AS total_theft_cellphone,\n","#    SUM(c.total_armed_robbery_cellphone) AS total_armed_robbery_cellphone,\n","#    SUM(c.total_theft_auto) AS total_theft_auto,\n","#    SUM(c.total_armed_robbery_auto) AS total_armed_robbery_auto\n","#FROM\n","#    crime c\n","#JOIN\n","#    filtered_segment s ON c.segment_id = s.id\n","#JOIN\n","#    filtered_timex t ON c.time_id = t.id;\n","#\"\"\")\n","query5 = spark.sql(\"\"\"SELECT\n","    SUM(total_armed_robbery_cellphone) AS total_roubo_celular,\n","    SUM(total_armed_robbery_auto) AS total_roubo_carro\n","FROM\n","    crime\n","WHERE time_id IN (\n","    SELECT id FROM timex WHERE year = 2017\n",");\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query5-1cpu-new.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query5.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query5-2cpu-new.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query5.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query5\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query5-new\"))\n","\n","#spark.stop()"],"metadata":{"id":"dJWnC0uKMJCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 6         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query6 = spark.sql(\"\"\"\n","SELECT\n","    c.segment_id,\n","    SUM(\n","        c.total_feminicide +\n","        c.total_homicide +\n","        c.total_felony_murder +\n","        c.total_bodily_harm +\n","        c.total_theft_cellphone +\n","        c.total_armed_robbery_cellphone +\n","        c.total_theft_auto +\n","        c.total_armed_robbery_auto\n","    ) AS total_crimes\n","FROM\n","    crime c\n","JOIN\n","    timex t ON c.time_id = t.id\n","WHERE\n","    t.month = 11\n","    AND t.year = 2010\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    total_crimes DESC;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query6-1cpu.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query6.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query6-2cpu.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query6.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query6\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query6\"))\n","\n","#spark.stop()"],"metadata":{"id":"gcGQ-93FODB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x0D-wJWC2VjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","#                          #\n","#       CONSULTA 7         #\n","#                          #\n","############################\n","\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"PolRoute-DS\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","query7 = spark.sql(\"\"\"\n","SELECT\n","    c.segment_id,\n","    SUM(\n","        c.total_feminicide +\n","        c.total_homicide +\n","        c.total_felony_murder +\n","        c.total_bodily_harm +\n","        c.total_theft_cellphone +\n","        c.total_armed_robbery_cellphone +\n","        c.total_theft_auto +\n","        c.total_armed_robbery_auto\n","    ) AS total_crimes\n","FROM\n","    crime c\n","JOIN\n","    timex t ON c.time_id = t.id\n","WHERE\n","    t.year = 2018\n","    AND t.weekday IN ('saturday', 'sunday')\n","GROUP BY\n","    c.segment_id\n","ORDER BY\n","    total_crimes DESC;\n","\"\"\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","output_file = os.path.join(output_dir, \"tempos-query7-1cpu.txt\") # Join directory and file name\n","\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-1cpu.txt\"\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # .show() mostrar os resultados\n","        query7.collect() # .collect() para simular execução real\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n","\n","# Nome do arquivo de saída\n","#output_file = \"/content/drive/MyDrive/data/results/tempos-query1-2cpu.txt\"\n","output_file = os.path.join(output_dir, \"tempos-query7-2cpu.txt\") # Join directory and file name\n","\n","with open(output_file, \"w\") as f:\n","    for i in range(1, 12):  # 1 a 11\n","        start = time.time()\n","\n","        #query1.show(1)  # Ou .collect() para simular execução real\n","        query7.collect()\n","\n","        end = time.time()\n","        duration = round(end - start, 3)\n","\n","        #print(f\"Running {i}: {duration} segundos\")\n","        f.write(f\"Running {i}: {duration}\\n\")\n","\n","# Para CSV\n","(query7\n"," .coalesce(1)  # Combina todos os dados em uma única partição\n"," .write\n"," .format(\"csv\")\n"," .option(\"header\", \"true\")\n"," .mode(\"overwrite\")\n"," .save(\"/content/drive/MyDrive/data/results/saida_query7\"))\n","\n","#spark.stop()"],"metadata":{"id":"YzkfnmCl2ekp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### EM CONSTRUCAO\n","\n","from pyspark.sql import SparkSession\n","import time\n","\n","# Criar sessão Spark (se ainda não tiver)\n","spark = SparkSession.builder \\\n","    .appName(\"MedirTempo\") \\\n","    .getOrCreate()\n","\n","# Configurar o número de partitions para simular paralelismo (com CPU)\n","#spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","# Nome do arquivo de saída\n","# Nome do arquivo de saída\n","output_dir = \"/content/drive/MyDrive/data/results/\" # Define the directory path\n","\n","for ncpu in range(1, 2):\n","\n","  # Configurar o número de partitions para simular paralelismo (com CPU)\n","  spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n","\n","  for index, query in enumerator(queries):\n","\n","    string = f\"tempos-query{index+1}-{ncpu}cpu.txt\"\n","    output_file = os.path.join(output_dir, string) # Join directory and file name\n","\n","    with open(output_file, \"w\") as f:\n","      for i in range(1, 2):  # 1 a 11\n","          start = time.time()\n","\n","          #query1.show(1)  # .show() mostrar os resultados\n","          query.collect() # .collect() para simular execução real\n","\n","          end = time.time()\n","          duration = round(end - start, 3)\n","\n","          #print(f\"Running {i}: {duration} segundos\")\n","          f.write(f\"Running {i}: {duration}\\n\")\n"],"metadata":{"id":"33g7xjk-iqrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n","mem_gib = mem_bytes/(1024.**3)  # e.g. 3.74\n","print(mem_gib)\n","cpuCount = os.cpu_count()\n","print(\"Number of CPUs in the system:\", cpuCount)\n","import multiprocessing\n","print(multiprocessing.cpu_count())\n","#cpu_info = !lscpu\n","#for inf_item in cpu_info.get_list():\n","#  print(inf_item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDP7mavY6C9E","executionInfo":{"status":"ok","timestamp":1750200679810,"user_tz":180,"elapsed":9,"user":{"displayName":"Carlos Thadeu Santos","userId":"06339206771126551680"}},"outputId":"8396642f-b5c8-4b66-850b-4f0f42f16ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12.673782348632812\n","Number of CPUs in the system: 2\n","2\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"17p2rjWbtK-q1_GPELuT8WDJPmJ8OJv47","timestamp":1750804307104}],"authorship_tag":"ABX9TyOVqWmcDfwblMNWr9oqE1qF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}